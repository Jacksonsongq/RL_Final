import gym
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Normal, Categorical
import matplotlib.pyplot as plt

# Actor Network for SAC
class SACActor(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(SACActor, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU()
        )
        self.mean = nn.Linear(64, action_dim)
        self.log_std = nn.Linear(64, action_dim)

    def forward(self, state):
        x = self.fc(state)
        mean = self.mean(x)
        log_std = self.log_std(x).clamp(-20, 2)
        return mean, log_std

# Critic Network for SAC
class SACCritic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(SACCritic, self).__init__()
        self.q_net = nn.Sequential(
            nn.Linear(state_dim + action_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, state, action):
        x = torch.cat([state, action], dim=-1)
        return self.q_net(x)

# Actor-Critic Network for PPO
class PPOActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(PPOActorCritic, self).__init__()
        self.actor = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim),
            nn.Softmax(dim=-1)
        )
        self.critic = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, state):
        return self.actor(state), self.critic(state)

# Replay Buffer for SAC
class ReplayBuffer:
    def __init__(self, size):
        self.buffer = []
        self.size = size

    def add(self, state, action, reward, next_state, done):
        if len(self.buffer) >= self.size:
            self.buffer.pop(0)
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        indices = torch.randint(0, len(self.buffer), (batch_size,))
        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in indices])
        return (
            torch.FloatTensor(states),
            torch.FloatTensor(actions),
            torch.FloatTensor(rewards),
            torch.FloatTensor(next_states),
            torch.FloatTensor(dones)
        )

def train_sac(env, episodes=500):
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]

    actor = SACActor(state_dim, action_dim)
    critic_1 = SACCritic(state_dim, action_dim)
    critic_2 = SACCritic(state_dim, action_dim)
    target_critic_1 = SACCritic(state_dim, action_dim)
    target_critic_2 = SACCritic(state_dim, action_dim)

    actor_optimizer = optim.Adam(actor.parameters(), lr=3e-4)
    critic_1_optimizer = optim.Adam(critic_1.parameters(), lr=3e-4)
    critic_2_optimizer = optim.Adam(critic_2.parameters(), lr=3e-4)

    replay_buffer = ReplayBuffer(100000)

    gamma = 0.99
    alpha = 0.2
    tau = 0.005

    rewards = []

    for episode in range(episodes):
        state = env.reset()
        state = torch.FloatTensor(state).unsqueeze(0)  # Ensure proper shape
        episode_reward = 0

        while True:
            state_tensor = state
            mean, log_std = actor(state_tensor)
            std = log_std.exp()
            dist = Normal(mean, std)
            action = dist.sample()
            action = action.clamp(env.action_space.low[0], env.action_space.high[0])

            next_state, reward, done, _ = env.step(action.squeeze(0).detach().numpy())
            next_state = torch.FloatTensor(next_state).unsqueeze(0)

            replay_buffer.add(state.squeeze(0).numpy(), action.squeeze(0).detach().numpy(), reward, next_state.squeeze(0).numpy(), done)
            state = next_state
            episode_reward += reward

            if len(replay_buffer.buffer) >= 64:
                states, actions, rewards_batch, next_states, dones = replay_buffer.sample(64)

                # Update Critic
                with torch.no_grad():
                    next_mean, next_log_std = actor(next_states)
                    next_std = next_log_std.exp()
                    next_dist = Normal(next_mean, next_std)
                    next_actions = next_dist.rsample()
                    next_q1 = target_critic_1(next_states, next_actions)
                    next_q2 = target_critic_2(next_states, next_actions)
                    next_q = torch.min(next_q1, next_q2) - alpha * next_dist.log_prob(next_actions).sum(dim=-1, keepdim=True)
                    target_q = rewards_batch.unsqueeze(-1) + gamma * (1 - dones.unsqueeze(-1)) * next_q

                q1 = critic_1(states, actions)
                q2 = critic_2(states, actions)

                critic_1_loss = nn.MSELoss()(q1, target_q)
                critic_2_loss = nn.MSELoss()(q2, target_q)

                critic_1_optimizer.zero_grad()
                critic_1_loss.backward()
                critic_1_optimizer.step()

                critic_2_optimizer.zero_grad()
                critic_2_loss.backward()
                critic_2_optimizer.step()

                # Update Actor
                mean, log_std = actor(states)
                std = log_std.exp()
                dist = Normal(mean, std)
                actions = dist.rsample()
                log_probs = dist.log_prob(actions).sum(dim=-1, keepdim=True)

                q1 = critic_1(states, actions)
                q2 = critic_2(states, actions)
                actor_loss = (alpha * log_probs - torch.min(q1, q2)).mean()

                actor_optimizer.zero_grad()
                actor_loss.backward()
                actor_optimizer.step()

                # Update Target Critics
                for target_param, param in zip(target_critic_1.parameters(), critic_1.parameters()):
                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)
                for target_param, param in zip(target_critic_2.parameters(), critic_2.parameters()):
                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)

            if done:
                break

        rewards.append(episode_reward)
        if episode % 50 == 0:
            print(f"SAC Episode {episode}, Reward: {episode_reward}")

    return rewards

def train_ppo(env, episodes=500):
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = PPOActorCritic(state_dim, action_dim)
    optimizer = optim.Adam(policy.parameters(), lr=3e-4)

    clip_epsilon = 0.2
    gamma = 0.99

    rewards = []

    for episode in range(episodes):
        state = env.reset()
        states, actions, rewards_batch, values, log_probs = [], [], [], [], []
        episode_reward = 0

        while True:
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            probs, value = policy(state_tensor)
            dist = Categorical(probs)
            action = dist.sample()

            next_state, reward, done, _ = env.step(action.item())

            states.append(state)
            actions.append(action)
            rewards_batch.append(reward)
            values.append(value.item())
            log_probs.append(dist.log_prob(action).item())

            state = next_state
            episode_reward += reward

            if done:
                break

        rewards.append(episode_reward)

        # Compute advantages and returns
        values.append(policy(torch.FloatTensor(state).unsqueeze(0))[1].item())
        advantages, returns = [], []
        gae = 0
        for i in reversed(range(len(rewards_batch))):
            delta = rewards_batch[i] + gamma * values[i + 1] - values[i]
            gae = delta + gamma * 0.95 * gae
            advantages.insert(0, gae)
            returns.insert(0, gae + values[i])

        # Update policy
        for _ in range(4):
            for state, action, log_prob, advantage, ret in zip(states, actions, log_probs, advantages, returns):
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                advantage_tensor = torch.FloatTensor([advantage])
                return_tensor = torch.FloatTensor([ret])

                probs, value = policy(state_tensor)
                dist = Categorical(probs)
                new_log_prob = dist.log_prob(action)
                ratio = torch.exp(new_log_prob - torch.FloatTensor([log_prob]))

                surr1 = ratio * advantage_tensor
                surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantage_tensor
                policy_loss = -torch.min(surr1, surr2).mean()

                value_loss = nn.MSELoss()(value, return_tensor)
                loss = policy_loss + 0.5 * value_loss

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

        if episode % 50 == 0:
            print(f"PPO Episode {episode}, Reward: {episode_reward}")

    return rewards

# Main comparison
env = gym.make("Pendulum-v1")

sac_rewards = train_sac(env, episodes=500)
ppo_rewards = train_ppo(env, episodes=500)

plt.plot(sac_rewards, label="SAC")
plt.plot(ppo_rewards, label="PPO")
plt.xlabel("Episodes")
plt.ylabel("Total Reward")
plt.title("SAC vs PPO Performance")
plt.legend()
plt.show()
